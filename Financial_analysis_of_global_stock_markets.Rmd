---
title: "Financial Analysis of Global Markets"
author: "Peter G"
date: "2023-05-10"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

## Background
The Standard and Poor's 500, or simply the S&P 500, is a stock market index tracking the stock performance of the 500 largest companies listed on stock exchanges in the United States.

The FTSE 100 is a major stock market index which tracks the performance of the 100 most capitalized companies traded on the London Stock Exchange. These companies represent about 80% of the entire market capitalization of the London Stock Exchange and it's a free-float index. It has a base value of 1000 since January 3, 1984.

The Hang Seng is a major stock market index which tracks the performance of around the 50 largest companies listed in the Stock Exchange of Hong Kong. It is free floating, capitalization-weighted index. The HSI has a base value of 100 since June 30, 1964.

The EURO STOXX 50 is a major stock market index which tracks the performance of 50 Blue-chip companies based in twelve Euro Area countries: Austria, Belgium, Finland, France, Germany, Greece, Ireland, Italy, Luxembourg, the Netherlands, Portugal and Spain.

The Nikkei 225 Stock Average Index is a major stock market index which tracks the performance of the 225 top rated companies listed on the Tokyo Stock Exchange. It is a price-weighted index. The Nikkei 225 has a base value of 176.21 since May 16, 1949.


### Data description

- The time series objects are ranging from 2018-01-04 for a five years period, until 2022-12-31.
- Five global market indices are analysed and predicted.
- The data contains daily observations.

### Objectives

First Section 

Comparative analysis of 5 global markets using 5 years data from 2018 to 2022

1. Short term returns over 5 years of period
2. Volatility in daily returns and risk calculations
3. Correlations among global market returns
4. Time taken to recover during pandemic

Second Section 
Predicting Open Direction for S&P500 market using other global indicators and market specific indicators.

1. Data preparation for prediction
2. Machine Learning algorithms to predict the opening direction of market

Third Section 
Pair Trading in technology sector - Identifying co-integrated pairs and time to entry/exit

1. Data collection from Yahoo Finance website
2. Performing Cointegration test
3. Automatic signal detection

The analysis also available through the developed Shiny application on the following address:
Shiny: https://ptrgska.shinyapps.io/financial_analysis_shiny/

```{r libraries}
# Import packages
pacman::p_load(quantmod, knitr, zoo, ggplot2, GGally, TSstudio, stringr,
               corrplot, ggcorrplot, PerformanceAnalytics, Hmisc, dplyr,
               reshape2, moments, urca, rpart, aTSA, egcm,caret, neuralnet,
               tidyverse, randomForest, bslib, TTR, e1071, shinyalert, car,
               performance, see, patchwork)
```

### Data import

Financial data has been downloaded from Yahoo Finance website.

```{r get_data}
# Create environment for stocks and function to download data.
ENV.STOCK <- new.env()
ticks.index <- c('^FTSE','^GSPC', '^HSI', '^SPEUP', '^N225')
sDate.index <- as.POSIXct('2018-01-04')
eDate.index <- as.POSIXct('2022-12-31')

readFinancialData <- function(ticker, envrmnt, start, end){
  data <- tryCatch(
            expr = {
              getSymbols(ticker,
                         src='yahoo',
                         from = start,
                         to = end,
                         periodicity = 'daily',
                         env = envrmnt)
            },
            error = function(e){
              message(paste('Please check the tickers are correct and try again.'))
              message(e)
              return(NA)
            },
            finally = {
              message(paste(" Processed ticker: ", ticker))
            }
          )
  return(data)
}

readFinancialData(ticks.index, ENV.STOCK, sDate.index, eDate.index)
```

## Data cleaning

Checking the number of missing entries from our financial data we obtained from yahoo. If there are missing values in the time series object, they will cause errors in the calculations, hence we use interpolation to get predicted values.

```{r cleaning}
# Clear the downloaded data from NA's.
for (i in ls(ENV.STOCK)){
  x <- get(i, envir = ENV.STOCK)
  print(paste0('The number of missing observations from ',
               i, ' is: ', sum(is.na(x))))
  if (sum(is.na(x)) != 0){
    ENV.STOCK[[i]] <- na.approx(x)
    print(paste0('Missing values are being interpolated for ', i, '.'))
  }
}
```

As we can see there were missing values in the data. We used spline interpolation to predict the values for the missing days.

```{r summary}
# Check for missing values and head of each stock.
for (i in ls(ENV.STOCK)){
      x <- get(i, envir = ENV.STOCK)
      print(ifelse(sum(is.na(x)) > 0, paste0('There are still some missing values in ',i),
                     paste0('There are no missing values in ',i,
                            ' , data has been successfully cleaned.')))
    }

lapply(X = ENV.STOCK, FUN = function(x) head(x))
```
The data has been cleaned and ready for further analysis. The first few rows of each downloaded index can be seen above.

### Time series

```{r merge}
# Create dataset for plot indices.
xts.adjusted <- do.call(merge, eapply(ENV.STOCK, Ad))
xts.adjusted <- na.locf(xts.adjusted)
xts.adjusted <- xts.adjusted[,c(2,1,5,3,4)]
    
for(i in colnames(xts.adjusted)){
    a <- str_locate(i, '.Adjusted')
    colnames(xts.adjusted)[which(names(xts.adjusted) == i)] <- str_sub(i,start = 1L, end = a[1,1]-1)
}
```

As the holiday and openings of the markets are different in North America, Europe and Asia, when combining the different data-sets, we inadvertently introduce Na's. Hence we replace each NA with the most recent value. This ensures that we use the last available value for the days when North-American or European markets were closed but Asia was trading. 

Once we have our data cleaned we can plot the time series, we use the adjusted values of the indices. 

The closing price is just the value of the last transacted price before the market closes, while the adjusted closing price factors in corporate actions, such as dividends, stock splits and rights offerings, hence it amends a stock's closing price to reflect the true value of the stock after calculating any corporate actions.

```{r adj_plots, fig.width = 12}
# Plot the time series of the indices.
lapply(X = xts.adjusted, FUN = function(x) plot.xts(x, 
       main = paste0('The price movement for the index ',colnames(x))))
```
Above the five global markets price movement over five years of period.

## Short term returns

### Daily returns

Daily returns of the markets over the five year period can be seen below. 

```{r daily_ret, fig.width = 12}
# Calculating daily return and plot for each indices.
xts.dailyreturns <- data.frame()
for (i in ls(ENV.STOCK)){
  x <- get(i, envir = ENV.STOCK)
  x2 <- round(dailyReturn(x)*100, 2)
  if(ncol(xts.dailyreturns) == 0){
    xts.dailyreturns = x2
  } else {
    xts.dailyreturns <- cbind(xts.dailyreturns, x2)
  }
  colnames(xts.dailyreturns)[ncol(xts.dailyreturns)] <- c(i)
}

par(mfrow=c(2,1))
lapply(X = xts.dailyreturns, FUN = function(x) 
       plot.xts(x, main = paste0('Simple daily return for the index ',colnames(x))))
```

We create a cumulative plot to show the prive movement of markets compare to each other over the years.
The cumulative return is the aggregate effect of price in the invested portfolio, it is the amount that the investment gained or lost over time. As the price fluctuates so does the return.

```{r cumprod, fig.width = 12}
# Calculate cumulative product of indices and plot data.
xts.cumulative.dailyret <- data.frame()
for (i in colnames(xts.adjusted)){
  x2 <- cumprod(1+dailyReturn(xts.adjusted[,i], type = 'arithmetic'))
  if(ncol(xts.cumulative.dailyret) == 0){
    xts.cumulative.dailyret = x2
  } else {
    xts.cumulative.dailyret = cbind(xts.cumulative.dailyret, x2)
  }
  colnames(xts.cumulative.dailyret)[ncol(xts.cumulative.dailyret)] <- c(i)
}

plot.xts(xts.cumulative.dailyret, main = 'The cumulative returns for the five indices',
         legend.names = c('FTSE', 'GSPC', 'SPEUP', 'N225', 'HSI' ),
         legend.loc = 'topleft',
         col=c('black', 'red', 'blue', 'lightblue', 'green'),
         lty=c(1, 1), lwd=c(2, 1)
         )
```

Average daily returns for each indices over the five years:

```{r average_daily}
# Calculate average daily returns.
for (i in colnames(xts.cumulative.dailyret)){
  print(paste0(i, ' mean: ' , mean(xts.cumulative.dailyret[,i], na.rm = TRUE)))
}
```

### Log returns

Calculating daily log returns and visualizing each of the series. The log returns are the percentage-wise increase or decrease in the price of a stock.
The log-return from one period is calculated as log(price at time t/price at time t-1) which is approximately equal to the percentage change in the price from time t-1 to time t.

```{r log_returns_daily, fig.width = 12}
# Create logreturn dataset and plot data.
xts.logreturns <- diff(log(xts.adjusted), lag = 1)
xts.logreturns <- na.omit(xts.logreturns)

par(mfrow=c(2,1))
lapply(X = xts.logreturns, FUN = function(x) 
       plot.xts(x, main = paste0('Daily log return for ',colnames(x))))
```

The monthly and annual log returns for each of the indices calculated. The table shows the annual log returns for each index by year.

```{r log_returns_annual_monthly}
# Create monthly and annual data-set for log returns.
xts.annuallogreturns <- data_frame()
xts.monthlylogreturns <- data.frame()

for (i in ls(ENV.STOCK)){
  y <- get(i, envir = ENV.STOCK)
  x <- coredata(annualReturn(y, type = 'log'))
  x2 <- monthlyReturn(y, type = 'log')
  if(ncol(xts.annuallogreturns) == 0 && ncol(xts.monthlylogreturns) == 0){
    xts.annuallogreturns = x
    xts.monthlylogreturns = x2
  } else {
    xts.annuallogreturns = cbind(xts.annuallogreturns, x)
    xts.monthlylogreturns = cbind(xts.monthlylogreturns, x2)
  }
  colnames(xts.annuallogreturns)[ncol(xts.annuallogreturns)] <- c(i)
  colnames(xts.monthlylogreturns)[ncol(xts.monthlylogreturns)] <- c(i)
}
kable(xts.annuallogreturns, caption = 'Annual log return of the indices.')
```

Visualizing the monthly mean with percentage-wise difference and returns of each stock. Also summarised in the table the yearly average return for each index.

```{r monthly_mean_perc_diff, fig.width=12}
# Mean plot of the indices.
xts.mon_mean <- apply.monthly(xts.adjusted, function(x) apply(x, 2, mean))
xts.monthlyreturns <- data.frame()
xts.yearlyreturns <- data.frame()

for (i in ls(ENV.STOCK)){
  x <- get(i, envir = ENV.STOCK)
  x2 <- round(monthlyReturn(x)*100, 2)
  x3 <- coredata(round(yearlyReturn(x)*100, 2))
  if(ncol(xts.monthlyreturns) == 0){
    xts.monthlyreturns = x2
    xts.yearlyreturns = x3
  } else {
    xts.monthlyreturns = cbind(xts.monthlyreturns, x2)
    xts.yearlyreturns = cbind(xts.yearlyreturns, x3)
  }
  colnames(xts.monthlyreturns)[ncol(xts.monthlyreturns)] <- c(i)
  colnames(xts.yearlyreturns)[ncol(xts.yearlyreturns)] <- c(i)
  
  plt <- chart_Series(xts.mon_mean[,i], TA=paste0('add_TA(xts.monthlyreturns$',i,',type = "h", name = "Monthly mean return over the 5 years period")'),
                    name = paste0('Monthly mean for ',i,' over the 5 years period'))
  print(plt)
}

kable(xts.yearlyreturns, caption = 'Yearly average returns of each index.')
```

### Volatility in markets and their respective daily returns

The volatility of a security is the standard deviation from its own daily return. It is used to measure the risk of a security.
We calculate the daily historical volatility that is a statistical measure of dispersion on returns.

```{r daily_volatility, fig.width = 12}
# Create volatility data-set and plot data.
xts.dailyvolatility <- data_frame()
for (i in ls(ENV.STOCK)){
  x <- get(i, envir = ENV.STOCK)
  x2 <- round(volatility(x, n = 3, calc = 'close'), 4)
  if(ncol(xts.dailyvolatility) == 0){
    xts.dailyvolatility = x2
  } else {
    xts.dailyvolatility <- cbind(xts.dailyvolatility, x2)
  }
  colnames(xts.dailyvolatility)[ncol(xts.dailyvolatility)] <- c(i)
}

par(mfrow=c(2,1))
lapply(X = xts.dailyvolatility, FUN = function(x) 
       plot.xts(x, main = paste0('Daily Volatility for ',colnames(x))))
```
The volatility plots for each market can be seen above. 

Calculating yearly volatility. Table below shows the yearly volatility of indices.

```{r yearly_volatility}
# Create annual volatility data-set.
df.yearlyvolatility <- data.frame()
for(i in colnames(xts.logreturns)){
  x <- apply.yearly(xts.logreturns[,i], sd)
  if(ncol(df.yearlyvolatility) == 0){
    df.yearlyvolatility = x * sqrt(252) * 100
  } else {
    df.yearlyvolatility = cbind(df.yearlyvolatility, x * sqrt(252) * 100)
  }
  colnames(df.yearlyvolatility)[ncol(df.yearlyvolatility)] <- c(i)
}

annualvola <- sapply(xts.logreturns, FUN = sd)
vec.yearlyvolatility <- round(annualvola * sqrt(252) * 100, 2)
print(paste0('The average annual percentage-wise volatility for the index ',
             c('FTSE', 'GSPC', 'HSI', 'N225', 'SPEUP'), ' ', vec.yearlyvolatility, '%'))

kable(df.yearlyvolatility, caption = 'Yearly volatility by index from 2018 to 2022')
```

Risk vs Return plot

```{r risk_return, fig.width = 12}
# Risk - Return plot
df.logret <- data.frame(coredata(xts.annuallogreturns))
df.logret <- as.data.frame(melt(lapply(df.logret, na.omit)))
df.yvola <- melt(coredata(df.yearlyvolatility))

df.logret[ncol(df.logret)+1] <- df.yvola[,3]

ggplot(data = df.logret, aes(x = value, y = V3))+
  geom_line(aes(linetype = L1, colour = L1))+
  geom_text(aes(label = round(V3, 2)), vjust = -1, size = 3)+
  ggtitle('Risk return plot of the global markets')+
  labs(x = 'Annual logreturn', y = 'Percentage change', color = 'Indices')

```

Value at risk (VaR) and expected shortfall (ES) calculations are important to understand the risk of an investment. It will show us what is the worst loss we can expect for a certain confidence interval and what is the expected return on average, in worst case scenario.
We first check our returns for normality.

```{r hist_skew_kurt, fig.width = 12}
# Normality test and histogram plot.
df.stat <- data.frame()

for(i in colnames(xts.logreturns)){
    plt <-  ggplot(data = xts.logreturns, aes(x = xts.logreturns[,i])) +
            geom_histogram(aes(y = ..density..),
                                 binwidth = 0.01,
                                 color = "black",
                                 fill = 4) +
            geom_density(size = 1.0) + 
            xlab('Daily Log Returns') +
            ylab('Density')+
            ggtitle(paste0(i,' Index'),
                    subtitle = paste0('Mean = ',round(mean(xts.logreturns[,i]),6),
                                      ' Std. Deviation = ', round(sd(xts.logreturns[,i]),4),
                                      ' Skewness: ', round(skewness(xts.logreturns[,i]),4),
                                      ' Kurtosis: ', round(kurtosis(xts.logreturns[,i]),4)))
    
  print(plt)
  jbt <- jarque.test(as.vector(xts.logreturns[,i]))
  swt <- shapiro.test(as.vector(xts.logreturns[,i]))
  
  new.row <- data.frame(Index = i,
                        Mean = round(mean(xts.logreturns[,i]),6),
                        Std.Deviation = round(sd(xts.logreturns[,i]),4),
                        Skewness = round(skewness(xts.logreturns[,i]),4),
                        Kurtosis = round(kurtosis(xts.logreturns[,i]),4),
                        Jarque.Bera.Test = c(jbt$statistic),
                        JB.p.value = c(jbt$p.value),
                        Shapriro.Wilk.Test = c(swt$statistic),
                        SW.p.value = c(swt$p.value))
                        
  df.stat <- rbind(df.stat, new.row)
}

kable(df.stat, caption = 'Statistics of the five idices.')
```

The distribution of log returns deviates from normality as each histograms of indices are leptokurtic and four of them is negatively-skewed, meaning there are more extremely negative values than positive ones. The HSI is the only one that has a symmetric distribution. This non-normality is supported by the Jarque-Bera and Shapiro-Wilks normality tests as well. Our indices are more volatile than if it had normal distribution.
As our data is asymmetric we, ruled out using normal distribution, hence we will simulate directly from an empirical distribution without making any assumption about it's shape.

- VaR - the quantile corresponding to alpha in the return distribution 

- ES - the average of all the returns to the left of the VaR

```{r VaR_ES}
# Value at Risk and Exprected Shortfall calculation.
df.VaR.ES <- data.frame()

for(i in colnames(xts.logreturns)){
    sample.emp <- sample(as.vector(xts.logreturns[,i]), 1000000, replace = TRUE)
    VaR.emp <- round(quantile(sample.emp, probs = 0.05), 6)
    ES.emp <- round(mean(sample.emp[sample.emp < VaR.emp]), 6)
    cat('Value at Risk for ',i,': ' , VaR.emp*100, '% and the Expected Shortfall: ',
        ES.emp*100, '%', fill = TRUE)
    pf <- 1000 # portfolio
    VaR.pf <- pf * (exp(VaR.emp) - 1)
    ES.pf <- pf * (exp(ES.emp) - 1)
    cat('Value at Risk for',i,': ' ,'in terms of $1000 investments is: ' ,
        VaR.pf, 'and the Expected Shortfall is: ', ES.pf, fill = TRUE)
    
     new.row <- data.frame(Index = i,
                           Values_at_Risk = VaR.emp,
                           Espected_Shortfal = ES.emp,
                           Value_at_Risk_Portfolio = VaR.pf,
                           Estimated_Shortfal_Portfolio = ES.pf)
     df.VaR.ES <- rbind(df.VaR.ES, new.row)
  }

kable(df.VaR.ES, caption = 'Value at Risk and Expected Shortfall for each index on average')
```

The table shows the Value at Risk and Estimated shortfal on average for an invested $1000. 

We don't expect to lose more than the value of Value at Risk at any given day with 95% confidence interval and in the worse 5% of cases we could expect a loss equal to the Expected Shortfall.

### Correlations among global markets and their daily returns

The correlation between the pairs of the daily returns of the five global markets.

```{r correlationPlot1, , fig.width = 12}
# Correlation plots - heatmap.
xts.daily.cor <- rcorr(as.matrix(na.approx(xts.dailyreturns)))
xts.monthly.cor <- rcorr(as.matrix(xts.monthlyreturns))
xts.yearly.cor <- rcorr(as.matrix(xts.yearlyreturns))

ggcorrplot(xts.daily.cor$r, hc.order = TRUE, outline.col = "white", 
           colors = c("blue", "white", "red"), lab = TRUE)+
      labs(x = NULL, y = NULL, title="Daily correlation")

chart.Correlation(na.approx(xts.dailyreturns), histogram=TRUE, pch = '+',
                  method = c("pearson", "kendall", "spearman"))

ggcorrplot(xts.monthly.cor$r, hc.order = TRUE, outline.col = "white",
           colors = c("blue", "white", "red"), lab = TRUE)+
      labs(x = NULL, y = NULL, title="Monthly correlation")

chart.Correlation(na.approx(xts.monthlyreturns), histogram=TRUE, pch = '+',
                  method = c("pearson", "kendall", "spearman"))

ggcorrplot(xts.yearly.cor$r, hc.order = TRUE, outline.col = "white",
           colors = c("blue", "white", "red"), lab = TRUE)+
      labs(x = NULL, y = NULL, title="Annual correlation")

chart.Correlation(na.approx(xts.yearlyreturns), histogram=TRUE, pch = '+',
                  method = c("pearson", "kendall", "spearman"))
```

The Pearson’s correlation coefficient, also known as Pearson’s r, describes the linear relationship between two quantitative variables. 

The assumptions the data must meet if we want to use Pearson’s r:
- Both variables are on an interval or ratio level of measurement.
- Data from both variables follow normal distributions. (It has been concluded that our data is skewed and not normally distributed.)
- Have no outliers.
- The data is from a random or representative sample.
- Expecting a linear relationship between the two variables.

The correlation between markets over a certain period shown below with a few combinations of markets and time frame.

```{r correlationPlot2, fig.width = 12}
# Correlation plots.
xts.dailyClose <- do.call(merge, eapply(ENV.STOCK, Cl))
xts.dailyClose <- na.locf(xts.dailyClose)

for(i in colnames(xts.dailyClose)){
  a <- str_locate(i, '.Close')
  colnames(xts.dailyClose)[which(names(xts.dailyClose) == i)] <- str_sub(i,start = 1L, end = a[1,1]-1)
}

xts.dailyClose <- as.data.frame(xts.dailyClose)
xts.dailyClose <-  xts.dailyClose %>% 
                   mutate(Cor = rollapply(cbind(xts.dailyClose[,1], xts.dailyClose[,2]),
                                           width = as.numeric(60),
                                           FUN = function(z) cor(z[,1],z[,2]),
                                           by.column = FALSE,
                                           fill = NA))
      
ggplot(xts.dailyClose, aes(x = index(xts.dailyClose), y = xts.dailyClose$Cor, group = 1))+
      geom_line()+
      geom_area(aes(fill = xts.dailyClose$Cor))+
      ggtitle(paste('Correlation between GSPC and FTSE by, 60 days period.'))+
      labs(x = 'Investigated period', y = 'Correlation positive/negative', fill = NULL)

xts.dailyClose <-  xts.dailyClose %>% 
                   mutate(Cor = rollapply(cbind(xts.dailyClose[,3], xts.dailyClose[,5]),
                                           width = as.numeric(90),
                                           FUN = function(z) cor(z[,1],z[,2]),
                                           by.column = FALSE,
                                           fill = NA))
      
ggplot(xts.dailyClose, aes(x = index(xts.dailyClose), y = xts.dailyClose$Cor, group = 1))+
      geom_line()+
      geom_area(aes(fill = xts.dailyClose$Cor))+
      ggtitle(paste('Correlation between N225 and HSI by, 90 days period.'))+
      labs(x = 'Investigated period', y = 'Correlation positive/negative', fill = NULL)

xts.dailyClose <-  xts.dailyClose %>% 
                    mutate(Cor = rollapply(cbind(xts.dailyClose[,1], xts.dailyClose[,5]),
                                           width = as.numeric(60),
                                           FUN = function(z) cor(z[,1],z[,2]),
                                           by.column = FALSE,
                                           fill = NA))
      
ggplot(xts.dailyClose, aes(x = index(xts.dailyClose), y = xts.dailyClose$Cor, group = 1))+
      geom_line()+
      geom_area(aes(fill = xts.dailyClose$Cor))+
      ggtitle(paste('Correlation between GSPC and HSI by, 60 days period.'))+
      labs(x = 'Investigated period', y = 'Correlation positive/negative', fill = NULL)

xts.dailyClose <-  xts.dailyClose %>% 
                    mutate(Cor = rollapply(cbind(xts.dailyClose[,4], xts.dailyClose[,2]),
                                           width = as.numeric(30),
                                           FUN = function(z) cor(z[,1],z[,2]),
                                           by.column = FALSE,
                                           fill = NA))
      
ggplot(xts.dailyClose, aes(x = index(xts.dailyClose), y = xts.dailyClose$Cor, group = 1))+
      geom_line()+
      geom_area(aes(fill = xts.dailyClose$Cor))+
      ggtitle(paste('Correlation between SPEUP and FTSE by, 30 days period.'))+
      labs(x = 'Investigated period', y = 'Correlation positive/negative', fill = NULL)
```
Scatter plot shown below for few of the possible combinations for correlation between the markets.

```{r correlationPlot3, fig.width = 12}
# Correlation plots - scatter.
df.dailyreturns <- as.data.frame(xts.dailyreturns)
      
ggplot(data = df.dailyreturns, aes(x = df.dailyreturns[,1], 
                                   y = df.dailyreturns[,2], 
                                   colour = df.dailyreturns[,1]))+
      geom_point()+
      labs(x = 'FTSE index', y = 'GSPC index') + 
      ggtitle('Correlation between FTSE and GSPC') +
      theme(plot.title = element_text(hjust = 0.5))+
      geom_smooth(method='loess',  linetype="dashed", 
                  color="red")+
      scale_colour_gradientn(colours = rainbow(3), 
                             name = paste('Colored by FTSE'))

ggplot(data = df.dailyreturns, aes(x = df.dailyreturns[,4], 
                                   y = df.dailyreturns[,3], 
                                   colour = df.dailyreturns[,4]))+
      geom_point()+
      labs(x = 'N225 index', y = 'HSI index') + 
      ggtitle('Correlation between N225 and HSI') +
      theme(plot.title = element_text(hjust = 0.5))+
      geom_smooth(method='loess',  linetype="dashed", 
                  color="red")+
      scale_colour_gradientn(colours = rainbow(3), 
                             name = paste('Colored by N225'))

ggplot(data = df.dailyreturns, aes(x = df.dailyreturns[,2], 
                                   y = df.dailyreturns[,5], 
                                   colour = df.dailyreturns[,2]))+
      geom_point()+
      labs(x = 'GSPC index', y = 'SPEUP index') + 
      ggtitle('Correlation between GSPC and SPEUP') +
      theme(plot.title = element_text(hjust = 0.5))+
      geom_smooth(method='loess',  linetype="dashed", 
                  color="red")+
      scale_colour_gradientn(colours = rainbow(3), 
                             name = paste('Colored by GSPC'))

ggplot(data = df.dailyreturns, aes(x = df.dailyreturns[,1], 
                                   y = df.dailyreturns[,5], 
                                   colour = df.dailyreturns[,1]))+
      geom_point()+
      labs(x = 'FTSE index', y = 'SPEUP index') + 
      ggtitle('Correlation between FTSE and SPEUP') +
      theme(plot.title = element_text(hjust = 0.5))+
      geom_smooth(method='loess',  linetype="dashed", 
                  color="red")+
      scale_colour_gradientn(colours = rainbow(3), 
                             name = paste('Colored by FTSE'))
```

### Time taken to recover during pandemic

On 20 February 2020, stock markets across the world suddenly crashed after growing instability due to the COVID-19 pandemic. The following plots show the period from the time of the crash until the price recovered to pre-pandemic levels with the time taken and the drop in value of the index.

```{r recovery, fig.width = 12}
# Recovery time calculations.
start.date = as.POSIXct('2020-02-20')
crash_value <- window(xts.adjusted, index = start.date)
kable(crash_value, caption = 'The values of the indices at the time of crash')

xts.adjusted.crash <- window(xts.adjusted, start = '2020-02-21')

for(i in colnames(xts.adjusted.crash)){
  x <- crash_value[,i]
    for(j in xts.adjusted.crash[,i]){
      if(j >= x){
        print(paste0('Stock ',i, ' crash value is ', x ,
                     ' and the recovery value ',j))
        print(paste0(' reached the same level at time ',
          index(xts.adjusted.crash[xts.adjusted.crash[,i] %in% j])))
        k <- index(xts.adjusted.crash[xts.adjusted.crash[,i] %in% j])
        print(paste0((k-index(crash_value)),
                     ' days has been ellapsed since the crash and recovery.'))
        plt <- plot.xts(xts.adjusted[,i],
                        subset = paste0('2020-02-20::',k),
                        main = paste0('Recovery time for ', i))
        break
      }
    }
  end.date = as.POSIXct(k)
  subdata <- window(xts.adjusted.crash[,i], start = start.date, end = end.date)
  print(paste0('The smallest market value of the crash was ',
               min(subdata), ' for the index ', i))
  print(paste0(', the Maximum Drowdrawn (MAXDD) was ', x-min(subdata) ,
               ' in price within the recovery period, that is '))
  print(paste0(round(100-((min(subdata)/x)*100), 2),'% drop in the price.'))
  print(plt)
}
```

## Predicting Open Direction for S&P500 market using other global indicators and market specific indicators

Trading stocks on any trading day takes an abrupt halt each afternoon when the markets close until next day, leaving uncertainty between then and the next day's open.
While the financial markets have clearly stated business hours, developments outside trading hours continue to influence both the value and investors action. Geopolitical events and natural disasters can occur at any time. Earnings announcements in companies made after the market closed or before it's opening can influence the market’s direction. During January, April, July, and October, majority of firms release their quarterly results that can change the direction of markets. 

After-hours trading activity is a common indicator of the next day's open, this extended-hours trading takes place on electronic communication networks known as ECNs before the financial markets open for the day, as well as after they close. Trading virtually 24 hours a day and also index futures can indicate how the market will likely trend at the start of the next day. Unlike the stock market, futures markets rarely close.
When one market is closed for the day, international markets are open and trading. A good day in Asian markets can suggest that U.S. markets will open higher but disastrous losses can lead to a lower open.

```{r ML_environment}
# Creating new environment for ML and downloading data.
ENV.ML <- new.env()
ticks.index <- c('^FTSE','^GSPC', '^HSI', '^SPEUP', '^N225')
sDate.index <- as.POSIXct('2018-01-04')
eDate.index <- as.POSIXct('2022-12-31')
readFinancialData(ticks.index, ENV.ML, sDate.index, eDate.index)

for (i in ls(ENV.ML)){
  x <- get(i, envir = ENV.ML)
  if (sum(is.na(x)) != 0){
    ENV.ML[[i]] <- na.approx(x)
  }
}
```

Preparing data for machine learning prediction.

```{r direction_data_collection}
# Assembling data-set for machine learning predictions.
df.GSPC <- ENV.ML$GSPC
ClToOp <- Cl(ENV.ML$GSPC) / Op(ENV.ML$GSPC)
HiToLow <- Hi(ENV.ML$GSPC)/Lo(ENV.ML$GSPC)
Vola <- round(volatility(ENV.ML$GSPC, n = 3, calc = 'close'), 4)
IndDir_N225 <- ifelse(dailyReturn(ENV.ML$N225) > 0,1,0)
IndDir_HSI <- ifelse(dailyReturn(ENV.ML$HSI) > 0,1,0)
IndDir_FTSE <- ifelse(((ENV.ML$FTSE$FTSE.Close - 
                        lag(ENV.ML$FTSE$FTSE.Close))/lag(ENV.ML$FTSE$FTSE.Close)) > 0,1,0)
IndDir_SPEUP <- ifelse(((ENV.ML$SPEUP$SPEUP.Close - 
                        lag(ENV.ML$SPEUP$SPEUP.Close))/lag(ENV.ML$SPEUP$SPEUP.Close)) > 0,1,0)
IndDir_GSPC <- ifelse(((ENV.ML$GSPC$GSPC.Close - 
                        lag(ENV.ML$GSPC$GSPC.Close))/lag(ENV.ML$GSPC$GSPC.Close)) > 0,1,0)

df.GSPC <- merge(df.GSPC, ClToOp, HiToLow, Vola, IndDir_GSPC,
                 IndDir_FTSE, IndDir_SPEUP, IndDir_HSI, IndDir_N225 )
df.GSPC <- na.locf(df.GSPC, fromLast = TRUE)
df.GSPC <- as.data.frame(df.GSPC)
colnames(df.GSPC)[7:14] <- c('GSPC.CloseToOpen','GSPC.HighToLow', 'GSPC.Volatility',
                             'GSPC.Direction', 'FTSE.Direction','SPEUP.Direction',
                             'HSI.Direction', 'N225.Direction')

df.GSPC <- df.GSPC %>% slice(-1)
df.GSPC <- na.omit(df.GSPC) 

index <- createDataPartition(df.GSPC$GSPC.Direction, p = 0.7, list = FALSE)
GSPC_train <- df.GSPC[index,]
GSPC_train <- na.locf(GSPC_train) 
GSPC_test <- df.GSPC[-index,]
GSPC_test <- na.locf(GSPC_test)

GSPC.Direction <- as.data.frame(GSPC_test$GSPC.Direction)
colnames(GSPC.Direction) <- c('GSPC_Direction') 
    
GSPC.formula <- as.formula(paste("GSPC.Direction ~ ",
                           paste(colnames(df.GSPC)[c(7, 8, 9, 11, 12, 13, 14)],
                           collapse = " + ")))
```

The directional plot for GSPC over the five years period shown below with the first few rows of the prepared data-set.

```{r direction_plot}
# Directional plot of GSPC.
z <- as.data.frame(table(df.GSPC$GSPC.Direction))
z<- z %>% mutate(prop = ((z[2]/length(df.GSPC$GSPC.Direction))*100))

ggplot(data = z, aes(x="", y = Freq, fill = Var1)) +
       geom_bar(width = 1, color = 'white', stat="identity")+
       coord_polar("y", start=0)+
       geom_text(aes(label = paste(round(prop$Freq, 2)," %")),
                 position = position_stack(vjust = 0.5))+
       ggtitle(paste0('Open/Close position for the index - GSPC' ))+
       labs(x = NULL, y = NULL, fill = NULL)+
       theme_classic() +
       theme(axis.line = element_blank(),
             axis.text = element_blank(),
             axis.ticks = element_blank())

head(df.GSPC)
```

### Binary Logistic Regression Model

Using a logistic regression to train our model and predict the possible opening direction of the market on a test set.

```{r BLR_pred, fig.width = 12, fig.height = 12}
# Binary Logistic Regression models and predictions.
model_BLR <- glm(GSPC.formula, 
                 data = GSPC_train, 
                 family = binomial)
summary(model_BLR)
 
model_BLR2 <- glm(GSPC.Direction ~ GSPC.CloseToOpen + SPEUP.Direction +
                                   FTSE.Direction + HSI.Direction + GSPC.HighToLow,
                 data = GSPC_train, 
                 family = binomial)
summary(model_BLR2)

# Model comparison with the intercept only model.
glm_intercept <- glm(GSPC.Direction~1, 
                     data = GSPC_train, 
                     family = binomial)

anova(glm_intercept, model_BLR2, test="Chisq")

# Prediction
GSPC.Direction$BLR.Pred <- ifelse(predict(model_BLR2,
                                          newdata = GSPC_test,
                                          type = 'response') > 0.6, 1, 0)

GSPC.Direction <- GSPC.Direction %>% 
                  mutate_at(c('GSPC_Direction','BLR.Pred'), as.factor)
confusionMatrix(GSPC.Direction$GSPC_Direction,
                GSPC.Direction$BLR.Pred,
                positive = '1')

print('Multicolinearity between the variables:')
vif(model_BLR2)

check_model(model_BLR2)
```

In our first model we can see that some of the variables are not significant hence the second corrected model. Multicolinearity is not present between the variables and the model has achieved over 87% accuracy.

### Decision Tree Model

```{r model_DT, fig.width = 12, fig.height = 9}
# Decision Tree models and predictions.
model_DT <- rpart(GSPC.formula,
                  data = GSPC_train, 
                  method = 'class',
                  parms = list(split='information'))

model_DT2 <- rpart(GSPC.Direction ~ GSPC.CloseToOpen + SPEUP.Direction + 
                                    FTSE.Direction + HSI.Direction + GSPC.HighToLow,
                   data = GSPC_train, 
                  method = 'class',
                  parms = list(split='information'))

plot(model_DT)
text(model_DT, splits = T, use.n = T, all = T, cex = 0.75)
importances <- varImp(model_DT)
importances %>% arrange(desc(Overall))

GSPC.Direction$DT.Pred <- predict(model_DT, newdata = GSPC_test, type = 'class')
GSPC.Direction$DT2.Pred <- predict(model_DT2, newdata = GSPC_test, type = 'class')

confusionMatrix(GSPC.Direction$GSPC_Direction, GSPC.Direction$DT.Pred, positive = '1')
confusionMatrix(GSPC.Direction$GSPC_Direction, GSPC.Direction$DT2.Pred, positive = '1')
```   

The Decision tree model prediction is over 80% with both combination of formulas.

### Random Forest Model

```{r RF_pred, fig.width = 12}
# Random Forest models and predictions.
set.seed(2)
model_RF <- randomForest(GSPC.formula,
                         data = GSPC_train,
                         mtry = 2,
                         ntree = 150,
                         importance = TRUE,
                         cutoff = c(0.6,0.4))

set.seed(5)
model_RF2 <- randomForest(GSPC.Direction ~ GSPC.CloseToOpen + SPEUP.Direction + 
                                           FTSE.Direction + HSI.Direction + GSPC.HighToLow,
                         data = GSPC_train,
                         mtry = 4,
                         ntree = 500,
                         importance = TRUE,
                         cutoff = c(0.6,0.4))

print(model_RF)
print(model_RF)
plot(model_RF)

print(model_RF2)
print(model_RF2)
plot(model_RF2)

GSPC.Direction$RF.Pred <- ifelse(predict(model_RF, GSPC_test, type = 'class') > 0.6, 1, 0)
GSPC.Direction$RF2.Pred <- ifelse(predict(model_RF2, GSPC_test, type = 'class') > 0.6, 1, 0)

GSPC.Direction$RF.Pred <- as.factor(GSPC.Direction$RF.Pred)
GSPC.Direction$RF2.Pred <- as.factor(GSPC.Direction$RF2.Pred)

confusionMatrix(GSPC.Direction$GSPC_Direction, GSPC.Direction$RF.Pred, positive = '1')
confusionMatrix(GSPC.Direction$GSPC_Direction, GSPC.Direction$RF2.Pred, positive = '1')
```

The random forest model has achieved an over 80% accuracy in both cases. Increasing the number of repetitions and the number of trees shows some improvements in the prediction.

### Artificial Neural Net Model

```{r ANN_pred, fig.width = 12}
# Artificial Neural Net models and predictions.
normalize <- function(x){
  return((x-min(x))/(max(x)-min(x)))
}
GSPC_train$GSPC.CloseToOpen <- normalize(GSPC_train$GSPC.CloseToOpen)
GSPC_train$GSPC.HighToLow <- normalize(GSPC_train$GSPC.HighToLow)
GSPC_train$GSPC.Volatility <- normalize(GSPC_train$GSPC.Volatility)

GSPC_test$GSPC.CloseToOpen <- normalize(GSPC_test$GSPC.CloseToOpen)
GSPC_test$GSPC.HighToLow <- normalize(GSPC_test$GSPC.HighToLow)
GSPC_test$GSPC.Volatility <- normalize(GSPC_test$GSPC.Volatility)

GSPC_train <- GSPC_train %>% mutate_at(c('GSPC.Direction', 'FTSE.Direction',
                                         'N225.Direction', 'SPEUP.Direction',
                                         'HSI.Direction'), as.numeric)
GSPC_test<- GSPC_test %>% mutate_at(c('GSPC.Direction', 'FTSE.Direction',
                                      'N225.Direction', 'SPEUP.Direction',
                                      'HSI.Direction'), as.numeric)

model_ANN <- neuralnet(GSPC.formula,
                       data = GSPC_train,
                       hidden = c(4,2),
                       linear.output = F,
                       threshold = 0.05,
                       rep = 2,
                       algorithm = "rprop+")

model_ANN2 <- neuralnet(GSPC.Direction ~ GSPC.CloseToOpen + SPEUP.Direction +
                        FTSE.Direction + HSI.Direction + GSPC.HighToLow,
                       data = GSPC_train,
                       hidden = c(3,4),
                       linear.output = F,
                       threshold = 0.05,
                       rep = 4)

plot(model_ANN, rep = 'best')
plot(model_ANN2, rep = 'best')

GSPC.Direction$ANN.Pred <- ifelse(predict(model_ANN, GSPC_test) > 0.6, 1, 0)
GSPC.Direction$ANN2.Pred <- ifelse(predict(model_ANN2, GSPC_test) > 0.6, 1, 0)

GSPC.Direction$ANN.Pred <- as.factor(GSPC.Direction$ANN.Pred)
GSPC.Direction$ANN2.Pred <- as.factor(GSPC.Direction$ANN2.Pred)

levels(GSPC.Direction$ANN.Pred) <- levels(GSPC.Direction$GSPC_Direction)
levels(GSPC.Direction$ANN2.Pred) <- levels(GSPC.Direction$GSPC_Direction)
confusionMatrix(GSPC.Direction$GSPC_Direction, GSPC.Direction$ANN.Pred, positive = '1')
confusionMatrix(GSPC.Direction$GSPC_Direction, GSPC.Direction$ANN2.Pred, positive = '1')
```

The neural net models both has a prediction of over 80%.

### Support Vector Machines Model

```{r SVM_pred}
# Support Vector Machines models and predictions.
GSPC_train$GSPC.Direction <- as.factor(GSPC_train$GSPC.Direction)
GSPC_test$GSPC.Direction <- as.factor(GSPC_test$GSPC.Direction)

model_SVM <- svm(GSPC.formula,
                      data = GSPC_train,
                      type = 'C-classification',
                      probability = TRUE,
                      kernel = 'linear',
                      cost = 10,
                      scale = FALSE
                      )

model_SVM2 <- svm(GSPC.Direction ~ GSPC.CloseToOpen + SPEUP.Direction +
                                   FTSE.Direction + HSI.Direction + GSPC.HighToLow,
                      data = GSPC_train,
                      type = 'nu-classification',
                      probability = TRUE,
                      kernel = 'polynomial',
                      cost = 10,
                      scale = FALSE
                      )

summary(model_SVM)
summary(model_SVM2)

GSPC.Direction$SVM.Pred <- predict(model_SVM, GSPC_test, probability = TRUE)
GSPC.Direction$SVM2.Pred <- predict(model_SVM2, GSPC_test, probability = TRUE)

confusionMatrix(GSPC.Direction$GSPC_Direction, GSPC.Direction$SVM.Pred, positive = '1')
confusionMatrix(GSPC.Direction$GSPC_Direction, GSPC.Direction$SVM2.Pred, positive = '1')
```

The support vector machine predictions are over 80% in both cases. 

All of the above tested models has an overall 80-90% accuracy rate.

## Pair Trading in technology:semiconductor industry - Identifying co-integrated pairs and time to entry/exit

The underlying logic of Pairs Trading is to monitor movements of co-integrated stocks and to look for trading opportunities when the divergence presents. Under the mean-reversion assumption, the stock price would tend to move back to the long-term equilibrium. As a result, the spread between two co-integrated stock prices would eventually converge. We can define a trading signal when to short or take a long position.

We investigate the Technology - Semiconductor industry that are listed on Nasdaq in the US. The stocks being investigated, all have been traded on the exchange from the time we conclude our analysis.

```{r technology_semiconductor}
# Creating environment for pair-trading and downloading data.
ENV.PAIR <- new.env()

ticks.pair <- c('AMD', 'AOSL', 'AMBA', 'AMKR', 'ASYS', 'ADI', 'AAOI',
                'ATOM', 'AXTI', 'BEEM', 'AVGO', 'CEVA','IMOS', 'CRUS',
                'CCMP', 'DIOD', 'EMKR', 'ENTG', 'FSLR', 'FORM', 'GSIT',
                'HIMX', 'ICHR', 'INTC','IPGP', 'ISUN', 'KLIC', 'LSCC',
                'MTSI', 'MRVL', 'MMAT', 'MCHP', 'MU', 'MPWR', 'MOSY',
                'LASR','NVEC', 'NVDA', 'NXPI', 'OIIM', 'ON', 'PLAB',
                'PXLW', 'POWI', 'QUIK', 'RMBS', 'RESN', 'RBCN','LEDS',
                'SMTC', 'SLAB', 'SIMO', 'SWKS', 'SGH', 'SEDG', 'SPI',
                'SPWR', 'SUNW', 'SPCB', 'TXN','TSEM', 'UCTT', 'OLED',
                'VECO', 'XLNX')
sDate.pair <- as.POSIXct('2019-01-01')
eDate.pair <- as.POSIXct('2022-12-31')

readFinancialData(ticks.pair, ENV.PAIR, sDate.pair, eDate.pair)

```

### Engle-Granger Test -Creating trading pairs and identifying the co-integration

Co-integration describes a long-term relationship between the prices.
The "integration” refers to an integrated time series of order d denoted as I(d) - integrated or differenced of order one time series denoted as I(1) (random walk). I(0) is a stationary time series, it implies that the mean and the variance of the time series are finite and do not change over time hence it's a time invariant property, meaning that if one series moves too far away from the mean the time invariant property will drag it back to the mean. 

By definition x_t and y_t are co-integrated, if x_t and y_t are I(1) series and there exists a beta such that z_t = x_t - beta*y_t is an I(0) series. 
This makes it possible to construct a stationary time series from two asset prices. 

Financial data are non-stationary time series. From two stocks, X and Y we perform a linear regression between them and check whether the residual is stationary using the Augmented Dick-Fuller (ADF) test. If the residual is stationary, then the two asset prices are co-integrated. The co-integration coefficient is obtained as the coefficient of the regressor. 

We take the closing prices of stocks and perform the Engle-Granger Test on each possible combination of pairs. The test performs the above explained logic  between the pairs and returns the test statistics and whether the pair is co-integrated. We create a data-frame with this information and then we filter out only those pairs that are co-integrated.

```{r pair_identification}
# Creating data-set for identification and performing test.
xts.close.pair <- do.call(merge, eapply(ENV.PAIR, Cl))

for(i in colnames(xts.close.pair)){
      if (sum(is.na(xts.close.pair[,i])) != 0){
        xts.close.pair[,i] <- na.approx(xts.close.pair[,i])
      }
  
      a <- str_locate(i, '.Close')
      colnames(xts.close.pair)[which(names(xts.close.pair) == i)] <- 
                              str_sub(i,start = 1L, end = a[1,1]-1)
}

xts.close.pair <- log(xts.close.pair)

df.allpairs <- allpairs.egcm(xts.close.pair)
df.cointegrated <- df.allpairs[df.allpairs$is.cointegrated == 'TRUE',]
```

```{r cointegrated _pairs_table}
head(df.cointegrated[,c(1,2,6:24)], 
     caption = 'The cointegrated trading pairs based on the Engle-Granger Test')
```

We choose a pair from our co-integrated data-frame, AAOI - GSIT and performing the analysis on the pair.

The plot shows the log of the closing price of the selected pair.
```{r pairs_plot, fig.width = 12}
# Pair time series plot.
chart_Series(xts.close.pair$AAOI, col = 'blue', type='line',
                 TA=c('add_TA(xts.close.pair$GSIT, col = "red", on = 1)'),
                 name = 'AAOI and TSEM, log plot over the investigated period')
```

The cointegrated pair-plot Price series with Residuals and Innovations, also shown the test summary.

```{r cointegrated_plot, fig.width = 12, fig.height = 12}
# Test plot and summary.
obj.pair <- egcm(xts.close.pair$AAOI, xts.close.pair$GSIT)
plot(egcm(xts.close.pair$AAOI, xts.close.pair$GSIT))
summary(egcm(xts.close.pair$AAOI, xts.close.pair$GSIT))
```

Normality and Partial Autocorrelation plots with scatter plot of the selected pair.

```{r normality_pacf_scatter_plot, fig.width = 12}
# Normality, pacf and scatter plot.
df.residuals <- as.data.frame(obj.pair$residuals)
colnames(df.residuals) <- c('residuals')
qqnorm(df.residuals$residuals, col = 'red')
qqline(df.residuals$residuals, distribution = qnorm, col = 'blue')
pacf(df.residuals$residuals)

xts.pairScatter <<- do.call(merge, eapply(ENV.PAIR, Cl))
    
for(i in colnames(xts.pairScatter)){
  a <- str_locate(i, '.Close')
  colnames(xts.pairScatter)[which(names(xts.pairScatter) == i)] <- 
    str_sub(i,start = 1L, end = a[1,1]-1)
}

df.pairScatter <- cbind(xts.pairScatter$AAOI, xts.pairScatter$GSIT)
df.pairScatter <- as.data.frame(df.pairScatter) 

ggplot(data = df.pairScatter, aes(x = xts.pairScatter$AAOI , y
                                  = df.pairScatter$GSIT, 
                                  color = df.pairScatter$AAOI))+
  geom_point()+
  geom_smooth(method = "lm", se = FALSE)+
  ggtitle('Scatter plot of the two cointegrated pairs and best fit regression line',
          subtitle = paste0('Standard Deviation of the residuals: ',sd(df.residuals$residuals)))+
  labs(x = 'AAOI', y = 'GSIT')+
  scale_colour_gradientn(colours = rainbow(4), name = 'Colored by AAOI')
```

Generating the data for the signal points.

```{r signalPoints}
# Creating signal points data-set based on choosen pair.
df.signalPoints <- data.frame()
counter_up <- 0
counter_down <- 0
percentsd <- (sd(df.residuals$residuals))*as.numeric(0.1)

for(i in df.residuals$residuals){
  if(i > as.numeric(1)*sd(df.residuals$residuals) & counter_up == 0){
    counter_up <- 1
    df.signalPoints <- rbind(df.signalPoints, list(match(i, df.residuals$residuals), i, 'Short')) 
  }
  else if(counter_up == 1 & i < percentsd){
    counter_up <- 0
    df.signalPoints <- rbind(df.signalPoints, list(match(i, df.residuals$residuals), i, 'Sell'))
  }
  else if(i < -as.numeric(1)*sd(df.residuals$residuals) & counter_down == 0){
    counter_down <- 1
    df.signalPoints <- rbind(df.signalPoints, list(match(i, df.residuals$residuals), i, 'Long'))
  }
  else if(counter_down == 1 & i > percentsd){
    counter_down <- 0
    df.signalPoints <- rbind(df.signalPoints, list(match(i, df.residuals$residuals), i, 'Sell'))
  }
}
colnames(df.signalPoints) <- c('Index', 'Value', 'Position')
```

The entry/exit plot for the pair AAOI-GSIT pair, where the red circles mark the signal points.

```{r entryExitPlot, fig.width = 12}
# Residuals plot with circles indicating the calculated signals.
ggplot(data = df.residuals, aes(x = index(df.residuals), y = df.residuals[,1] ))+
  geom_line()+
  geom_hline(yintercept = 0, linetype='dashed', 
             color = 'red', 
             linewidth = 0.25)+
  geom_hline(yintercept = c(-1*sd(df.residuals$residuals), 
                            1*sd(df.residuals$residuals),
                            percentsd, -percentsd),
             linetype='dashed', 
             color = 'blue', 
             linewidth = 0.25)+
  geom_point(data = df.signalPoints,
             aes(x = df.signalPoints[,1],
                 y = df.signalPoints[,2]), 
             pch = 21, 
             size = 4, 
             color = 'red')+
  theme_bw()+
  ggtitle('Residuals plot with signal points to entry exit from trade')+
  labs(x = 'Timescale from first to last index', y = 'Residuals')

kable(df.signalPoints, caption = 'The signal point for the AAOI-GSIt pair.')
```
